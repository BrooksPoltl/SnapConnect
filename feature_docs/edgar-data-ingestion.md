# **Feature Design: EDGAR Data Ingestion for RAG**

## **1. Overview**

**Purpose:** This document outlines the design and implementation plan for
sourcing, processing, and indexing U.S. Securities and Exchange Commission (SEC)
filings. This data pipeline is the foundational backend component for the
AI-powered caption generation feature.

**Problem:** To provide accurate, data-driven AI captions, the application
requires a reliable and searchable knowledge base of corporate financial
documents. Manually searching these documents is not feasible for a real-time AI
feature.

**Solution:** We will implement a two-phased pipeline using a set of Python
scripts.

- **Phase 1:** Fetch public `10-K` and `10-Q` filings from the SEC's EDGAR
  database and store their textual content locally.
- **Phase 2:** Process this local data by chunking it, generating vector
  embeddings using OpenAI, and upserting it into a dedicated Pinecone index for
  efficient semantic search.

**Success Outcome:** A robust, searchable Pinecone vector index containing the
textual content of recent financial filings from key public companies.

## **2. System Architecture**

The ingestion pipeline is designed to be a standalone, script-based workflow,
intentionally decoupled from the main React Native application codebase.

- **Data Source:** SEC EDGAR Database, accessed via the `edgartools` Python
  library.
- **Processing Engine:** A new `scripts/` directory in the project root will
  contain all Python scripts, dependencies (`requirements.txt`), and environment
  configurations.
- **Intermediary Storage:** Raw text from filings will be temporarily stored in
  `scripts/filings_data/` as structured JSON files. This creates a checkpoint,
  allowing us to re-run the embedding and upserting process without
  re-downloading all the data from the SEC.
- **Vector Database:** Pinecone. Data will be pushed directly to our Pinecone
  index using the `pinecone-client` SDK, which is the most direct method and
  avoids the overhead of object storage (like S3) and Parquet file formatting.
- **Embeddings Service:** OpenAI's `text-embedding-3-small` model will be used
  to generate vector embeddings for our text chunks.

## **3. Data Model & Schema**

Defining a clear data structure is critical for a successful RAG pipeline.

#### **A. Local JSON File Schema (`/filings_data/*.json`)**

Each file represents one downloaded SEC filing. This schema is for Phase 1.

```json
{
  "ticker": "AAPL",
  "form_type": "10-K",
  "filing_date": "2023-11-03",
  "accession_number": "0000320193-23-000106",
  "report_url": "https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20231103.htm",
  "sections": {
    "Item 1. Business": "Text content of the business section...",
    "Item 1A. Risk Factors": "Text content of the risk factors section..."
  }
}
```

#### **B. Pinecone Vector Schema**

Each vector in Pinecone represents a single chunk of text from a section of a
larger document. This is the target schema for Phase 2.

- **Vector ID:** A composite key to ensure uniqueness and allow for
  document-level operations. Format:
  `"{accession_number}#{section_title_safe}#chunk_{chunk_index}"`.
  - _Example:_ `"0000320193-23-000106#Item_1A_Risk_Factors#chunk_001"`
- **Vector Values:** The numerical embedding generated by OpenAI.
- **Vector Metadata:** A JSON object containing the searchable context.

```json
{
  "ticker": "AAPL",
  "form_type": "10-K",
  "filing_date": "2023-11-03",
  "report_url": "https://www.sec.gov/...",
  "section_title": "Item 1A. Risk Factors",
  "chunk_index": 1,
  "chunk_text": "The text content of this specific chunk..."
}
```

## **4. Implementation Plan**

### Phase 1: Fetch Filings and Store Text Locally

**Goal**: Create a reusable script to download the textual content of SEC
filings for specific companies, structured by section.

| Priority | Task Description                 | Implementation Details                                                                                                                                                            | Dependencies        |
| :------- | :------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------ |
| High     | Setup Python script environment. | Create `scripts/` dir, `requirements.txt` (`edgartools`, `python-dotenv`, `tqdm`), and `.env.example`.                                                                            | None                |
| High     | Create data fetching script.     | `scripts/fetch_filings.py` will take company tickers as arguments and save output to `scripts/filings_data/`.                                                                     | Environment Setup   |
| High     | Implement EDGAR data fetching.   | Loop through tickers, get last 2 years of `10-K` & `10-Q` filings. For each, use `filing.sections()` to get a dictionary of sections. Save as a JSON file using the schema above. | Script Structure    |
| Medium   | Add logging and documentation.   | Add logging for progress/errors and a `scripts/README.md` explaining how to run the script.                                                                                       | Script Finalization |

### Phase 2: Process and Upsert Data to Pinecone

**Goal**: Create a script to process the locally stored JSON files, generate
embeddings for each section chunk, and upsert them into Pinecone.

| Priority | Task Description                      | Implementation Details                                                                                                                                                                      | Dependencies                        |
| :------- | :------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------- |
| High     | Enhance script environment.           | Add `pinecone-client` and `openai` to `requirements.txt`. Update `.env` with Pinecone/OpenAI keys.                                                                                          | Phase 1                             |
| High     | Create Pinecone upsert script.        | `scripts/upsert_to_pinecone.py` will read the JSON files from `scripts/filings_data/`.                                                                                                      | Pinecone Project, OpenAI API Access |
| High     | Implement text chunking.              | For each section in each JSON file, split the text into smaller, overlapping chunks suitable for embedding.                                                                                 | Script Structure                    |
| High     | Implement embedding and upsert logic. | For each chunk, generate an embedding using OpenAI. Upsert the vector to Pinecone using the SDK's `upsert()` method with the schema defined above. Process in batches to manage API limits. | Text Chunking                       |
| Medium   | Add logging and documentation.        | Update `scripts/README.md` to explain the upsert process.                                                                                                                                   | Script Finalization                 |

## **5. Future Considerations**

- **Automation:** The current design is manual. A future iteration could deploy
  these scripts as a scheduled cloud function (e.g., Supabase Edge Function with
  a cron job or a GitHub Action) to automatically ingest new filings daily.
- **Granular Ingestion:** While `filing.sections()` is very powerful, some
  filings may have important information in other attachments. The Phase 2
  script could be enhanced to optionally parse key exhibits (e.g. `EX-99`) from
  `filing.attachments` if needed.
