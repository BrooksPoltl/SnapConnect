# **Feature Design: EDGAR Data Ingestion for RAG**

## **1. Overview**

**Purpose:** This document outlines the design and implementation plan for sourcing, processing, and indexing U.S. Securities and Exchange Commission (SEC) filings. This data pipeline is the foundational backend component for the AI-powered caption generation feature.

**Problem:** To provide accurate, data-driven AI captions, the application requires a reliable and searchable knowledge base of corporate financial documents. Manually searching these documents is not feasible for a real-time AI feature.

**Solution:** We will implement a two-phased pipeline using a set of Python scripts.
*   **Phase 1:** Fetch public `10-K` and `10-Q` filings from the SEC's EDGAR database and store their textual content locally.
*   **Phase 2:** Process this local data by chunking it, generating vector embeddings using OpenAI, and upserting it into a dedicated Pinecone index for efficient semantic search.

**Success Outcome:** A robust, searchable Pinecone vector index containing the textual content of recent financial filings from key public companies.

## **2. System Architecture**

The ingestion pipeline is designed to be a standalone, script-based workflow, intentionally decoupled from the main React Native application codebase.

*   **Data Source:** SEC EDGAR Database, accessed via the `edgartools` Python library.
*   **Processing Engine:** A new `scripts/` directory in the project root will contain all Python scripts, dependencies (`requirements.txt`), and environment configurations.
*   **Intermediary Storage:** Raw text from filings will be temporarily stored in `scripts/filings_data/` as structured JSON files. This creates a checkpoint, allowing us to re-run the embedding and upserting process without re-downloading all the data from the SEC.
*   **Vector Database:** Pinecone. Data will be pushed directly to our Pinecone index using the `pinecone-client` SDK, which is the most direct method and avoids the overhead of object storage (like S3) and Parquet file formatting.
*   **Embeddings Service:** OpenAI's `text-embedding-3-small` model will be used to generate vector embeddings for our text chunks.

## **3. Data Model & Schema**

Defining a clear data structure is critical for a successful RAG pipeline.

#### **A. Local JSON File Schema (`/filings_data/*.json`)**
Each file represents one downloaded SEC filing. This schema is for Phase 1.

```json
{
  "ticker": "AAPL",
  "form_type": "10-K",
  "filing_date": "2023-11-03",
  "accession_number": "0000320193-23-000106",
  "report_url": "https://www.sec.gov/Archives/edgar/data/320193/000032019323000106/aapl-20231103.htm",
  "sections": {
      "Item 1. Business": "Text content of the business section...",
      "Item 1A. Risk Factors": "Text content of the risk factors section..."
  }
}
```

#### **B. Pinecone Vector Schema**
Each vector in Pinecone represents a single chunk of text from a section of a larger document. This is the target schema for Phase 2.

*   **Vector ID:** A composite key to ensure uniqueness and allow for document-level operations. Format: `"{accession_number}#{section_title_safe}#chunk_{chunk_index}"`.
    *   *Example:* `"0000320193-23-000106#Item_1A_Risk_Factors#chunk_001"`
*   **Vector Values:** The numerical embedding generated by OpenAI.
*   **Vector Metadata:** A JSON object containing the searchable context.

```json
{
  "ticker": "AAPL",
  "form_type": "10-K",
  "filing_date": "2023-11-03",
  "report_url": "https://www.sec.gov/...",
  "section_title": "Item 1A. Risk Factors",
  "chunk_index": 1,
  "chunk_text": "The text content of this specific chunk..."
}
```

## **4. Implementation Plan**

### Phase 1: Fetch Filings and Store Text Locally

**Goal**: Create a reusable script to download the textual content of SEC filings for specific companies, structured by section.

| Priority | Task Description | Implementation Details | Dependencies |
| :--- | :--- | :--- | :--- |
| High | Setup Python script environment. | Create `scripts/` dir, `requirements.txt` (`edgartools`, `python-dotenv`, `tqdm`), and `.env.example`. | None |
| High | Create data fetching script. | `scripts/fetch_filings.py` will take company tickers as arguments and save output to `scripts/filings_data/`. | Environment Setup |
| High | Implement EDGAR data fetching. | Loop through tickers, get last 2 years of `10-K` & `10-Q` filings. For each, use `filing.sections()` to get a dictionary of sections. Save as a JSON file using the schema above. | Script Structure |
| Medium | Add logging and documentation. | Add logging for progress/errors and a `scripts/README.md` explaining how to run the script. | Script Finalization |

### Phase 2: Process and Upsert Data to Pinecone

**Goal**: Create a script to process the locally stored JSON files, generate embeddings for each section chunk, and upsert them into Pinecone.

| Priority | Task Description | Implementation Details | Dependencies |
| :--- | :--- | :--- | :--- |
| High | Enhance script environment. | Add `pinecone-client` and `openai` to `requirements.txt`. Update `.env` with Pinecone/OpenAI keys. | Phase 1 |
| High | Create Pinecone upsert script. | `scripts/upsert_to_pinecone.py` will read the JSON files from `scripts/filings_data/`. | Pinecone Project, OpenAI API Access |
| High | Implement text chunking. | For each section in each JSON file, split the text into smaller, overlapping chunks suitable for embedding. | Script Structure |
| High | Implement embedding and upsert logic. | For each chunk, generate an embedding using OpenAI. Upsert the vector to Pinecone using the SDK's `upsert()` method with the schema defined above. Process in batches to manage API limits. | Text Chunking |
| Medium | Add logging and documentation. | Update `scripts/README.md` to explain the upsert process. | Script Finalization |

## **5. Future Considerations**

*   **Automation:** The current design is manual. A future iteration could deploy these scripts as a scheduled cloud function (e.g., Supabase Edge Function with a cron job or a GitHub Action) to automatically ingest new filings daily.
*   **Granular Ingestion:** While `filing.sections()` is very powerful, some filings may have important information in other attachments. The Phase 2 script could be enhanced to optionally parse key exhibits (e.g. `EX-99`) from `filing.attachments` if needed. 